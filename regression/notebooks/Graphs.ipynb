{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        # fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(nn.Linear(1, 2))\n",
    "        self.fc_layers.append(nn.Linear(2, 2))\n",
    "        self.fc_layers.append(nn.Linear(2, 1))\n",
    "        \n",
    "        for layer in self.fc_layers:\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.fill_(1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for k in range(len(self.fc_layers) - 1):\n",
    "            x = F.relu(self.fc_layers[k](x))\n",
    "        y = self.fc_layers[-1](x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1]) tensor([[[1],\n",
      "         [2],\n",
      "         [3],\n",
      "         [4]]]) torch.LongTensor\n",
      "tensor([[[ 1],\n",
      "         [ 4],\n",
      "         [ 9],\n",
      "         [16]]]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)+1\n",
    "x = x.view(1, -1, 1)\n",
    "y = x**2\n",
    "print(x.shape, x, x.type())\n",
    "print(y, y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11.],\n",
      "         [15.],\n",
      "         [19.],\n",
      "         [23.]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 1],\n",
      "         [ 4],\n",
      "         [ 9],\n",
      "         [16]]])\n"
     ]
    }
   ],
   "source": [
    "# forward pass through network \n",
    "model = TestModel()\n",
    "yhat = model(x.float())\n",
    "print(yhat)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(92.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = F.mse_loss(y.float(), yhat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_layers.0.weight tensor([[90.],\n",
      "        [90.]])\n",
      "fc_layers.0.bias tensor([38., 38.])\n",
      "fc_layers.1.weight tensor([[64., 64.],\n",
      "        [64., 64.]])\n",
      "fc_layers.1.bias tensor([19., 19.])\n",
      "fc_layers.2.weight tensor([[147., 147.]])\n",
      "fc_layers.2.bias tensor([19.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 92.5\n",
      "fc_layers.0.weight tensor([[180.],\n",
      "        [180.]])\n",
      "fc_layers.0.bias tensor([76., 76.])\n",
      "fc_layers.1.weight tensor([[128., 128.],\n",
      "        [128., 128.]])\n",
      "fc_layers.1.bias tensor([38., 38.])\n",
      "fc_layers.2.weight tensor([[294., 294.]])\n",
      "fc_layers.2.bias tensor([38.])\n"
     ]
    }
   ],
   "source": [
    "# now what is gradient if we just do the same thing?\n",
    "# forward pass through network \n",
    "yhat = model(x.float())\n",
    "loss = F.mse_loss(y.float(), yhat)\n",
    "print(f'loss: {loss}')\n",
    "loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_layers.0.weight tensor([[0.],\n",
      "        [0.]])\n",
      "fc_layers.0.bias tensor([0., 0.])\n",
      "fc_layers.1.weight tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "fc_layers.1.bias tensor([0., 0.])\n",
      "fc_layers.2.weight tensor([[0., 0.]])\n",
      "fc_layers.2.bias tensor([0.])\n",
      "loss: 92.5\n",
      "first pass--------------------------\n",
      "fc_layers.0.weight tensor([[90.],\n",
      "        [90.]])\n",
      "fc_layers.0.bias tensor([38., 38.])\n",
      "fc_layers.1.weight tensor([[64., 64.],\n",
      "        [64., 64.]])\n",
      "fc_layers.1.bias tensor([19., 19.])\n",
      "fc_layers.2.weight tensor([[147., 147.]])\n",
      "fc_layers.2.bias tensor([19.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8483e1a80ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'second pass-------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Apps/anaconda3/envs/fleet/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Apps/anaconda3/envs/fleet/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# now do again, but keep the graph\n",
    "# but first zero the gradient\n",
    "model.zero_grad()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)\n",
    "yhat = model(x.float())\n",
    "loss = F.mse_loss(y.float(), yhat)\n",
    "print(f'loss: {loss}')\n",
    "loss.backward(create_graph=False)\n",
    "print('first pass--------------------------')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)\n",
    "loss.backward(create_graph=False)\n",
    "print('second pass-------------------------')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
